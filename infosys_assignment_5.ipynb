{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aplha27/Infosys_internship/blob/main/infosys_assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBrukRfPGggy",
        "outputId": "5e2a269e-5e3d-4e23-fa01-df6c81d5fbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FlBVbAlOXcy",
        "outputId": "50b329dc-258f-4083-b426-ed9fa4d70176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.2.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSR7H4bkSNcQ",
        "outputId": "5a6bcea8-3351-481a-c87b-249f430015b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_bpScANAZy",
        "outputId": "97169e2b-a01e-4264-9504-21baa0a02d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data_path = 'cleaned_data.xlsx'\n",
        "df = pd.read_excel(data_path)\n",
        "\n",
        "# Feature extraction functions\n",
        "def count_words(text):\n",
        "    return len(text.split()) if pd.notnull(text) else 0\n",
        "\n",
        "def count_characters(text):\n",
        "    return len(text) if pd.notnull(text) else 0\n",
        "\n",
        "def avg_word_length(text):\n",
        "    words = text.split() if pd.notnull(text) else []\n",
        "    return sum(len(word) for word in words) / len(words) if words else 0\n",
        "\n",
        "def count_sentences(text):\n",
        "    return len(re.split(r'[.!?]', text)) - 1 if pd.notnull(text) else 0\n",
        "\n",
        "def count_uppercase_ratio(text):\n",
        "    return sum(1 for char in text if char.isupper()) / len(text) if pd.notnull(text) and len(text) > 0 else 0\n",
        "\n",
        "def keyword_count(text, keywords):\n",
        "    words = text.split() if pd.notnull(text) else []\n",
        "    return sum(1 for word in words if word.lower() in keywords)\n",
        "\n",
        "def unique_word_ratio(text):\n",
        "    words = text.split() if pd.notnull(text) else []\n",
        "    return len(set(words)) / len(words) if words else 0\n",
        "\n",
        "def check_role_in_resume(resume, role):\n",
        "    return 1 if pd.notnull(resume) and role.lower() in resume.lower() else 0\n",
        "\n",
        "def keyword_overlap(text1, text2):\n",
        "    if pd.notnull(text1) and pd.notnull(text2):\n",
        "        words1 = set(text1.split())\n",
        "        words2 = set(text2.split())\n",
        "        return len(words1 & words2)\n",
        "    return 0\n",
        "\n",
        "# Predefined keyword dictionaries\n",
        "technical_keywords = {'python', 'java', 'sql', 'machine learning', 'cloud', 'design', 'analysis', 'management'}\n",
        "positive_keywords = {'excellent', 'success', 'outstanding', 'achievement', 'skilled'}\n",
        "negative_keywords = {'poor', 'inadequate', 'lacking', 'failure', 'weak'}\n",
        "\n",
        "# Feature engineering\n",
        "df['resume_word_count'] = df['Cleaned_Resume'].apply(count_words)\n",
        "df['resume_char_count'] = df['Cleaned_Resume'].apply(count_characters)\n",
        "df['resume_avg_word_length'] = df['Cleaned_Resume'].apply(avg_word_length)\n",
        "df['resume_sentence_count'] = df['Cleaned_Resume'].apply(count_sentences)\n",
        "df['resume_uppercase_ratio'] = df['Cleaned_Resume'].apply(count_uppercase_ratio)\n",
        "df['resume_technical_keyword_count'] = df['Cleaned_Resume'].apply(lambda x: keyword_count(x, technical_keywords))\n",
        "df['resume_positive_keyword_count'] = df['Cleaned_Resume'].apply(lambda x: keyword_count(x, positive_keywords))\n",
        "df['resume_negative_keyword_count'] = df['Cleaned_Resume'].apply(lambda x: keyword_count(x, negative_keywords))\n",
        "df['resume_unique_word_ratio'] = df['Cleaned_Resume'].apply(unique_word_ratio)\n",
        "\n",
        "df['transcript_word_count'] = df['Cleaned_Transcript'].apply(count_words)\n",
        "df['transcript_char_count'] = df['Cleaned_Transcript'].apply(count_characters)\n",
        "df['transcript_avg_word_length'] = df['Cleaned_Transcript'].apply(avg_word_length)\n",
        "df['transcript_sentence_count'] = df['Cleaned_Transcript'].apply(count_sentences)\n",
        "df['transcript_uppercase_ratio'] = df['Cleaned_Transcript'].apply(count_uppercase_ratio)\n",
        "df['transcript_positive_keyword_count'] = df['Cleaned_Transcript'].apply(lambda x: keyword_count(x, positive_keywords))\n",
        "df['transcript_negative_keyword_count'] = df['Cleaned_Transcript'].apply(lambda x: keyword_count(x, negative_keywords))\n",
        "df['transcript_unique_word_ratio'] = df['Cleaned_Transcript'].apply(unique_word_ratio)\n",
        "\n",
        "df['job_role_in_resume'] = df.apply(lambda row: check_role_in_resume(row['Cleaned_Resume'], row['Role']), axis=1)\n",
        "\n",
        "df['resume_job_keyword_overlap'] = df.apply(lambda row: keyword_overlap(row['Cleaned_Resume'], row['Cleaned_Job_Description']), axis=1)\n",
        "df['transcript_job_keyword_overlap'] = df.apply(lambda row: keyword_overlap(row['Cleaned_Transcript'], row['Cleaned_Job_Description']), axis=1)\n",
        "\n",
        "# Role popularity (frequency encoding)\n",
        "role_counts = df['Role'].value_counts()\n",
        "df['role_popularity'] = df['Role'].map(role_counts)\n",
        "\n",
        "# Decision reason encoding\n",
        "df['decision_reason_encoded'] = df['Reason for decision'].astype('category').cat.codes\n",
        "\n",
        "# Embedding extraction\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to compute embeddings in batches\n",
        "def compute_embeddings(texts):\n",
        "    inputs = tokenizer(texts.tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "\n",
        "# Extract embeddings for each column\n",
        "resume_embeddings = compute_embeddings(df['Cleaned_Resume'].fillna(\"\"))\n",
        "transcript_embeddings = compute_embeddings(df['Cleaned_Transcript'].fillna(\"\"))\n",
        "job_description_embeddings = compute_embeddings(df['Cleaned_Job_Description'].fillna(\"\"))\n",
        "\n",
        "# Combine embeddings\n",
        "embedding_features = np.hstack([resume_embeddings, transcript_embeddings, job_description_embeddings])\n",
        "\n",
        "# Additional features\n",
        "engineered_features = [\n",
        "    'transcript_positive_keyword_count', 'resume_positive_keyword_count',\n",
        "    'transcript_avg_word_length', 'decision_reason_encoded',\n",
        "    'transcript_char_count', 'transcript_job_keyword_overlap',\n",
        "    'resume_negative_keyword_count', 'resume_job_keyword_overlap',\n",
        "    'resume_char_count', 'transcript_unique_word_ratio'\n",
        "]\n",
        "additional_features = df[engineered_features].values\n",
        "\n",
        "# Combine all features\n",
        "X = np.hstack([embedding_features, additional_features])\n",
        "y = df['decision'].apply(lambda x: 1 if x == 'select' else 0).values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_params = {'colsample_bytree': 0.6, 'learning_rate': 0.05, 'max_depth': 5,\n",
        "              'n_estimators': 300, 'subsample': 1.0}\n",
        "xgb_model = XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Neural Network\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Ensemble the models\n",
        "ensemble_model = VotingClassifier(estimators=[('xgb', xgb_model), ('mlp', mlp)], voting='soft')\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Ensemble Model Accuracy: {accuracy}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPMvQXkxHBBU7hScgUPKKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}